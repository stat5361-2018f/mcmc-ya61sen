---
title: "Homework 7 - STAT 5361 Statistical Computing"
author:
  - Sen Yang^[<sen.2.yang@uconn.edu>; M.S. student at
    Department of Statistics, University of Connecticut.]
date: "`r format(Sys.time(), '%d %B %Y')`"
documentclass: article
papersize: letter
fontsize: 11pt
bibliography: template.bib
biblio-style: asa
output:
  bookdown::pdf_document2
abstract: |
    This is homework 7 for STAT 5361 - Statistical Computing.
---


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
## some utility functions, see the source code for details
source("utils_template.R")

## specify the packages needed
pkgs <- c("DT", "leaflet", "splines2", "webshot")
need.packages(pkgs)

## external data can be read in by regular functions,
## such as read.table or load

## for latex and html output
isHtml <- knitr::is_html_output()
isLatex <- knitr::is_latex_output()
latex <- ifelse(isLatex, '\\LaTeX\\', 'LaTeX')

## specify global chunk options
knitr::opts_chunk$set(fig.width = 5, fig.height = 4, dpi = 300,
                      out.width = "90%", fig.align = "center")

```


# Markov Chain Monte Carlo sampling{#sec:6.3.1}

## Posterior inference of Normal mixture

For a Normal mixture of $N(\mu_1,\sigma_1^2)$ and $N(\mu_2,\sigma_2^2)$ with propotion of first Normal distribution $\delta$, we have
$$X_i\sim\delta N(\mu_1,\sigma_1^2)+(1-\delta)N(\mu_1,\sigma_1^2)$$
Then the likelihood function of $X_i|\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\delta$ is given as
$$L(X_i|\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\delta)=\prod_{i=1}^n [\delta\cdot \phi(x_i|\mu_1,\sigma_1^2)+(1-\delta)\cdot\phi(x_i|\mu_1,\sigma_1^2)]$$
which can be rewritten as
$$f(x|\theta)\varpropto\prod_{i=1}^n [\frac{\delta}{\sigma_1}\exp(\frac{(x_i-\mu_1)^2}{\sigma_1^2})+\frac{1-\delta}{\sigma_2}\exp(\frac{(x_i-\mu_2)^2}{\sigma_2^2})]$$
Since the prior for $1/\sigma_1^2$ and $1/\sigma_1^2$ are $\Gamma(a,b)$ with shape $a=0.5$ and scale $b=10$, then $\sigma_1^2 \sim$ $Inv$-$Gamma(a,1/b)$ and $\sigma_2^2 \sim$ $Inv$-$Gamma(a,1/b)$.
Also, the prior for $\mu_1$ and $\mu_2$ are $N(0,10^2)$ and all the priors are independent. The posterior density of $\theta=(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\delta)$ is

\begin {align*}
q(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\delta|x)\varpropto \prod_{i=1}^n [\frac{\delta}{\sigma_1}\exp(\frac{(x_i-\mu_1)^2}{\sigma_1^2})&+\frac{1-\delta}{\sigma_2}\exp(\frac{(x_i-\mu_2)^2}{\sigma_2^2})] \cdot \exp(\mu_1^2) \cdot \exp(\mu_2^2) \cdot \\ &(\sigma_1^2)^{-1.5}\exp(\frac1{\sigma_1^2}) \cdot (\sigma_2^2)^{-1.5}\exp(\frac1{\sigma_2^2})
\end {align*}

The full conditional distributions of $\theta=(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\delta)$ can be shown to be log-concave, which allows adaptive rejection alogorithm.

## MCMC process in R

* Firstly, generate some data from a mixtrure Normal of of $N(-1,0.1)$ and $N(5,0.5)$ with propotion of first Normal distribution $\delta=0.7$.

```{r ran_sample, echo=T}
## generate a random sample from a mixture normal with true parameter values
delta <- 0.7 # true value to be estimated based on the data
n <- 100
set.seed(123)
u <- rbinom(n, prob = delta, size = 1)
x <- rnorm(n, ifelse(u == 1, 7, 10) , 0.5)
mydata <- data.frame(x)
```

* Secondly, calculate the posterior density.

```{r post_d, echo=T}
## posterior density
## posterior density
logpost <- function(theta,data) {
  mu_1 <- theta[1]; mu_2 <- theta[2]; 
  sigma2_1 <- theta[3]; sigma2_2 <- theta[4]; delta <- theta[5]
  x <- data$x
  library("invgamma")
  return(sum(log(delta*dnorm(x,mu_1,sigma2_1^0.5)+(1-delta)*dnorm(x,mu_2,sigma2_2^0.5))) +
           dnorm(mu_1,0,10,log = T) + dnorm(mu_2,0,10,log = T) +
           dinvgamma(sigma2_1,shape=0.5,scale=10,log = T) +
           dinvgamma(sigma2_2,shape=0.5,scale=10,log = T))
}
```

* Thirdly, An MCMC based the Gibbs sampler uses the ARMS algorithm from R package **HI**.

```{r mcmc, echo=T}
## MCMC
mymcmc <- function(niter, thetaInit, data, nburn) {
  p <- length(thetaInit)
  thetaCurrent <- thetaInit
  ## define a function for full conditional sampling  
  logFC <- function(th, idx) {
    theta <- thetaCurrent
    theta[idx] <- th
    logpost(theta, data)
  }
  out <- matrix(thetaInit, niter, p, byrow = TRUE)
  ## Gibbs sampling
  for (i in 2:niter) {
    for (j in 1:p) {
      ## general-purpose arms algorithm
      # Indicator function
      indF <- function(x, idx) {
        if (idx==1 | idx==2) {(x<30)*(x>-30)}
        else if (idx==3 | idx==4) {(x<10)*(x>0)}
        else {(x<1)*(x>=0)}
      }
      out[i, j] <- thetaCurrent[j] <-
        HI::arms(thetaCurrent[j], logFC, indF, 1, idx = j)
    }
  }
  out[-(1:nburn), ]
}
```

* Finally, run a simulation, drop the first 1000 observations and draw corresponding histogram for all parameters.
```{r simu, echo=T}
## simulation
niter <- 5000; nburn <- 1000
thetaInit <- c(0,0,0.2,0.2,0.5)
sim <- mymcmc(niter, thetaInit, mydata, nburn)
name_para <- c("mu_1","mu_2","sigma2_1","sigma2_2","delta")
for (i in 1:5) {
  plot(ts(sim[,i]),main=(paste(name_para[i])))
  hist(sim[,i],main=(paste(name_para[i])))
}
```




